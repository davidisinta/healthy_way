{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOccLqT+KIBz+cYQBfcTltr",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/davidisinta/healthy_way/blob/main/food_classifier.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "food classifier"
      ],
      "metadata": {
        "id": "7Fxso0oSnPDn"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xMiULRPcnOp_"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "download the dataset if it doesnt exist"
      ],
      "metadata": {
        "id": "YupX8BdooDOa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from __future__ import absolute_import, division, print_function\n",
        "\n",
        "import cv2\n",
        "import os\n",
        "import random\n",
        "import collections\n",
        "from collections import defaultdict\n",
        "from shutil import copy\n",
        "from shutil import copytree, rmtree\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as img\n",
        "%matplotlib inline\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow.keras.backend as K\n",
        "from tensorflow.keras.models import load_model\n",
        "from tensorflow.keras.utils import load_img, img_to_array\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.preprocessing import image as kimage\n",
        "from tensorflow.keras import regularizers\n",
        "from tensorflow.keras.applications.resnet50 import ResNet50\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Dense, Dropout, Input, GlobalAveragePooling2D\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, CSVLogger\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.regularizers import l2\n",
        "\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import models\n",
        "from tensorflow.keras.applications.resnet50 import decode_predictions, preprocess_input\n",
        "\n",
        "from google.colab.patches import cv2_imshow\n"
      ],
      "metadata": {
        "id": "yblZq4KRoCog"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Helper function to download data and extract\n",
        "\n",
        "def get_data_extract():\n",
        "  if \"food-101\" in os.listdir():\n",
        "    print(\"Dataset already exists\")\n",
        "  else:\n",
        "    tf.keras.utils.get_file(\n",
        "    'food-101.tar.gz',\n",
        "    'http://data.vision.ee.ethz.ch/cvl/food-101.tar.gz',\n",
        "    cache_subdir='/content',\n",
        "    extract=True,\n",
        "    archive_format='tar',\n",
        "    cache_dir=None\n",
        "    )\n",
        "    print(\"Dataset downloaded and extracted!\")"
      ],
      "metadata": {
        "id": "VsD-UE-koJie"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Download data and extract it to folder\n",
        "get_data_extract()"
      ],
      "metadata": {
        "id": "yfyqyiAloO4g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Visualize random data from the Dataset"
      ],
      "metadata": {
        "id": "5Vbb7RRVoUWB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rows = 17\n",
        "cols = 6\n",
        "# this choice of rows and columns is because the product results in 102, and there are 101 classes\n",
        "\n",
        "\n",
        "fig, ax = plt.subplots(rows, cols, figsize=(25,25))\n",
        "# subplots not to be mistaken with subplot\n",
        "\n",
        "fig.suptitle(\"Showing one random image for each class\", y=1.05, fontsize=24)\n",
        "# Adding  y=1.05, fontsize=24 helped me fix the suptitle overlapping with axes issue\n",
        "\n",
        "# let's grab our image directory. We'll name the directory data_dir\n",
        "data_dir = '/content/food-101/images'\n",
        "\n",
        "food_sorted = sorted(os.listdir(data_dir)) # start from inner function\n",
        "\n",
        "# create a food_id counter to exit the for loop\n",
        "food_id = 0\n",
        "\n",
        "# now let's loop through the row and column length we specified\n",
        "\n",
        "for i in range(rows): # this will make the loop exit when the number of rows are exhausted\n",
        "  for j in range(cols):\n",
        "    try:\n",
        "      # now we select the alphabetically listed food classes(folder names) in order\n",
        "      food_selected= food_sorted[food_id]\n",
        "      food_id+=1\n",
        "    except:\n",
        "      break\n",
        "\n",
        "    # let's grab the list of all files present in each food category\n",
        "    food_selected_images = os.listdir(os.path.join(data_dir, food_selected))\n",
        "\n",
        "    food_selected_random = np.random.choice(food_selected_images)\n",
        "\n",
        "\n",
        "    img = plt.imread(os.path.join(data_dir, food_selected, food_selected_random))\n",
        "\n",
        "    # Let's now display these randomly selected images\n",
        "    ax[i][j].imshow(img)\n",
        "    ax[i][j].set_title(food_selected, pad=10)\n",
        "\n",
        "plt.setp(ax, xticks=[], yticks=[])\n",
        "plt.tight_layout()\n"
      ],
      "metadata": {
        "id": "RK7lrnz_oXl4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Split data into train and test sets"
      ],
      "metadata": {
        "id": "nUV9gAU5ocHS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Helper method to split the images into trainng and test folders\n",
        "def train_test_cleave(filepath, src, dest):\n",
        "\n",
        "  img_classes = defaultdict(list)\n",
        "\n",
        "  # open up the filepath\n",
        "  # use list comprehension to strip the link of whitespaces\n",
        "  # create a map of the images\n",
        "  with open(filepath, 'r') as txt:\n",
        "    paths = [read.strip() for read in txt.readlines()]\n",
        "    for p in paths:\n",
        "      food = p.split('/')\n",
        "      img_classes[food[0]].append(food[1] + '.jpg')\n",
        "\n",
        "    from shutil import copy\n",
        "\n",
        "    # iterate over the food keys in the img_classes dictionary\n",
        "  for food in img_classes:\n",
        "    print(\"\\n Copying images into {} ...\".format(food))\n",
        "    if not os.path.exists(os.path.join(dest,food)):\n",
        "      os.makedirs(os.path.join(dest,food))\n",
        "    for i in img_classes[food]:\n",
        "      # import:: from shutil import copy\n",
        "      copy(os.path.join(src,food,i), os.path.join(dest,food,i))\n",
        "  print(\"Copy complete...\")"
      ],
      "metadata": {
        "id": "iFyEiOzFoeac"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Copy images from food-101/images to food-101/train based on the train.txt\n",
        "print(\"\\n Creating training set...\")\n",
        "train_test_cleave('/content/food-101/meta/train.txt', 'food-101/images', 'food-101/train')"
      ],
      "metadata": {
        "id": "T8J_HKaOoo4E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Copy imageas from food-101/images to food-101/test based on the test.txt\n",
        "print(\"\\n Creating test set...\")\n",
        "train_test_cleave('/content/food-101/meta/test.txt', 'food-101/images', 'food-101/test')"
      ],
      "metadata": {
        "id": "HzepL3A_ovh4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "use a subset of the food 101 dataset that contains only 4 classes because it takes lots of compute to train 101 classes"
      ],
      "metadata": {
        "id": "dhRoLxT-o44N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mini_list = [\"caesar_salad\", \"steak\", \"lasagna\", \"fried_rice\"]\n",
        "src_train = \"food-101/train\"\n",
        "dest_train = \"food-101/train_mini\"\n",
        "src_test = \"food-101/test\"\n",
        "dest_test = \"food-101/test_mini\""
      ],
      "metadata": {
        "id": "AYx7wsAopAEP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def sample_datset(food_list, src, dest):\n",
        "  if os.path.exists(dest):\n",
        "    # removing dataset_mini(if it already exists) folders so that we will have only the classes that we want\n",
        "    rmtree(dest)\n",
        "  os.makedirs(dest)\n",
        "\n",
        "  for food_item in food_list:\n",
        "    print(\"Copying images from {} to {}/{} ...\".format(src, dest, food_item))\n",
        "    copytree(os.path.join(src, food_item), os.path.join(dest,food_item))\n",
        "    print(\"Done!\")"
      ],
      "metadata": {
        "id": "rPctrODVpF-A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Creating train data folder with 4 new classes\")\n",
        "sample_datset(mini_list, src_train, dest_train)"
      ],
      "metadata": {
        "id": "hnBOsRsjpTU_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Creating test data folder with new classes\")\n",
        "sample_datset(mini_list, src_test, dest_test)"
      ],
      "metadata": {
        "id": "NgTiElJ6pWQ7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Total number of samples in train folder\")\n",
        "train_files = sum([len(files) for i, j, files in os.walk(\"food-101/train_mini\")])\n",
        "print(train_files)"
      ],
      "metadata": {
        "id": "gsGaovzbphiw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Total number of samples in test folder\")\n",
        "test_files = sum([len(files) for i, j, files in os.walk(\"food-101/test_mini\")])\n",
        "print(test_files)"
      ],
      "metadata": {
        "id": "AyJ4BGqApkZy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Fine tune ResNet50 on Food101 dataset"
      ],
      "metadata": {
        "id": "pgfZ64YKpnXS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(n_categories, num_epochs, num_train_samples, num_validation_samples):\n",
        "\n",
        "  K.clear_session() # we clear the session to provide a faster impplemenmtation of the model\n",
        "\n",
        "  img_width, img_height = 299, 299\n",
        "  train_data_dir = \"food-101/train_mini\"\n",
        "  validation_data_dir = \"food-101/test_mini\"\n",
        "  batch_size = 20\n",
        "  bestmodel_path = \"bestmodel_\" + str(n_categories)+\"classes.keras\"\n",
        "  trainedmodel_path = 'trainedmodel_'+ str(n_categories)+ \"class.keras\"\n",
        "  history_path = 'history_'+str(n_categories)+'.log'\n",
        "\n",
        "  train_datagen = ImageDataGenerator(\n",
        "      rotation_range=40,\n",
        "      width_shift_range=0.2,\n",
        "      height_shift_range=0.2,\n",
        "      rescale=1. / 255,\n",
        "      shear_range=0.2,\n",
        "      zoom_range=0.2,\n",
        "      horizontal_flip=True,\n",
        "      fill_mode='nearest'\n",
        "  )\n",
        "\n",
        "  test_datagen = ImageDataGenerator(rescale=1. / 255)\n",
        "\n",
        "  train_generator = train_datagen.flow_from_directory(\n",
        "      train_data_dir,\n",
        "      target_size=(img_height, img_width),\n",
        "      batch_size=batch_size,\n",
        "      class_mode='categorical'\n",
        "  )\n",
        "\n",
        "  validation_generator = test_datagen.flow_from_directory(\n",
        "      validation_data_dir,\n",
        "      target_size=(img_height, img_width),\n",
        "      batch_size=batch_size,\n",
        "      class_mode='categorical'\n",
        "  )\n",
        "\n",
        "\n",
        "  # Cool. Now let's create an instance of our Resnet50 model\n",
        "  resnet_awesome = ResNet50(include_top=False, weights='imagenet')\n",
        "  x = resnet_awesome.output\n",
        "  x = GlobalAveragePooling2D()(x)\n",
        "  x = Dense(64, activation='relu')(x)\n",
        "  x = Dropout(0.2)(x)\n",
        "\n",
        "\n",
        "  #Let's create an object to hold our predictions.\n",
        "  predictions = Dense(n_categories, kernel_regularizer=regularizers.l2(0.0049), activation='softmax')(x)\n",
        "  # The softmax function is an activation function that turns numbers into probabilities which sum to one.\n",
        "  # The softmax function outputs a vector that represents the probability distributions of a list of outcomes\n",
        "  #It is useful for finding out the class which has the max. Probability.\n",
        "  #The Softmax function is ideally used in the output layer,\n",
        "  # where we are actually trying to attain the probabilities to define the class of each input.\n",
        "\n",
        "  # We now have the basic architecture for fine-tuning the Resnet50. But we cant train it this way.\n",
        "  # What we now do is create an instance of the Model. We do this by [assing our architecture into\n",
        "  # the Model class, which takes in the details of the input and output of our model architecture.\n",
        "  # Let's import the Model class now.\n",
        "\n",
        "  model = Model(inputs=resnet_awesome.input, outputs=predictions)\n",
        "  model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=[\"accuracy\"])\n",
        "\n",
        "  # since we are working with a multi-class problem (which should not be confused with multi-label problems),\n",
        "  # we make use of categorical_crossentropy loss and the softmax function.\n",
        "  # If on the other hand we were dealing with a multi-label problem, we would then probably use\n",
        "  # binary_crossentropy and the sigmoid activation for the final layer.\n",
        "\n",
        "\n",
        "  # Let's now create checkpoint and csv error logger callbacks\n",
        "\n",
        "  # A callback is a powerful tool to customize the behavior of a Keras model during training,\n",
        "  # evaluation, or inference, including reading/changing the Keras model.\n",
        "  # Callback is a python class meant to be subclassed to provide specific functionality,\n",
        "  # with a set of methods called at various stages of training (including batch/epoch start and ends),\n",
        "  # testing, and predicting. In this case, we will be using built-in callbacks.\n",
        "  # However, in subsequent tutorials, I will be going over how you can build your own custom callback classes.\n",
        "\n",
        "\n",
        "  checkpoint = ModelCheckpoint(filepath=bestmodel_path, verbose=1, save_best_only=True)\n",
        "\n",
        "  # This creates a saved version of our best performing model.\n",
        "  # setting verbose=0 means verbose is false. This means we wont see any output during training\n",
        "\n",
        "  csv_logger = CSVLogger(history_path)\n",
        "\n",
        "  # The CSVLogger contains the loss value, and all the metrics at the end of a batch or epoch\n",
        "\n",
        "  # Use floor division operator // to return floor value\n",
        "  # for both integer and floating point arguments.\n",
        "  # In python, the floor of x is the largest integer not greater than x\n",
        "\n",
        "  history = model.fit(train_generator,\n",
        "                                steps_per_epoch = num_train_samples // batch_size,\n",
        "                                validation_data=validation_generator,\n",
        "                                validation_steps=num_validation_samples//batch_size,\n",
        "                                epochs=num_epochs,\n",
        "                                verbose =1,\n",
        "                                callbacks=[csv_logger, checkpoint])\n",
        "  ## Call the save method\n",
        "  model.save(trainedmodel_path)\n",
        "  # The dictionary containing the mapping from class names to class indices\n",
        "  # can be obtained via the attribute \"class_indices\"\n",
        "  class_map = train_generator.class_indices\n",
        "  return history, class_map"
      ],
      "metadata": {
        "id": "RRWN_dIOpqbx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train the Model"
      ],
      "metadata": {
        "id": "_J9ujCOyp0_o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model with data from 4 classes\n",
        "n_classes = 4\n",
        "epochs = 1\n",
        "num_train_samples = train_files\n",
        "num_validation_samples = test_files\n",
        "\n",
        "history, class_map_4 = train_model(n_classes, epochs, num_train_samples, num_validation_samples)\n",
        "print(class_map_4)"
      ],
      "metadata": {
        "id": "dy1TXhaSp2n7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Predicting classes for new images from internet images using our best trained model"
      ],
      "metadata": {
        "id": "4_ZCR3JxqFGO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the best saved model to make predictions\n",
        "\n",
        "K.clear_session()\n",
        "## import:: from tensorflow.keras.models import load_model\n",
        "model_best = load_model(\"/content/trainedmodel_4class.keras\", compile=False)"
      ],
      "metadata": {
        "id": "7JdBsNlKqF1t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_class(model, picture, show = True, rescale=1. / 255):\n",
        "  pic = kimage.load_img(picture, target_size=(299, 299))\n",
        "  pic = kimage.img_to_array(pic)\n",
        "  pic = np.expand_dims(pic, axis=0)\n",
        "  pic *= rescale\n",
        "  #pic = preprocess_input(pic)\n",
        "\n",
        "  pred = model.predict(pic)\n",
        "  index = np.argmax(pred)\n",
        "\n",
        "  mini_list.sort()\n",
        "  pred_value = mini_list[index]\n",
        "\n",
        "  if show:\n",
        "      plt.imshow(pic[0])\n",
        "      plt.axis('off')\n",
        "      plt.title(pred_value)\n",
        "      plt.show()\n",
        ""
      ],
      "metadata": {
        "id": "TBPHYnL1qN24"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "imagepath = '/content/food-101/test_mini/lasagna/1184609.jpg'\n",
        "predict_class(model_best, imagepath, True)"
      ],
      "metadata": {
        "id": "9Rtb_tyAqUsM"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}